The swapchain represents a queue of images (framebuffers) that the GPU draws
into and then presents to the screen
Each of the images represents an available image that can be
    - rendered to
    - presented to the display

Normally systems have the following configurations:
    - double buffering
        one image is presented while the other one is being rendered to
    - triple buffering
        smoother, normally used in the following way:
            - CPU issues draw commands to one
            - The GPU renders another one
            - The presentation engine displays it
    - 3+
        rare but often used in VR systems for high frame-rate targets

Initialization
    - Query the system's swapchain
    - Query the available images (double buffered? triple buffered? n+ buffered?)
      this just tells you the available image slots, not some pre-allocated image, that's up
      to the application
    - Create a container of VkImages to fill each of the slots with 
    - Create an image view per each of the VkImages available

redux
    A swapchain is an abstraction over an array of images that are associated in a way
    with an specific surface, these images are VkImages and are created by the platform

    Components
        Presentation Engine
        Window
        Surface
        Swapchain
    The window is the native to the platform, it supports a Vulkan Surface where one
    can attach a vulkan swapchain to it

    Presentation Engine
        Abstraction for the platform's compositor or display engine
        Some implementations use the graphics queues or a separate presentation hardware
        to perform the presentation

    Workflow
        - create the swapchain by requesting how the images will look like
            "The capabilities of a swapchain targeting a surface are the intersection of
             capabilities of the WSI platform, the native window or display, and the
             physical device"
            - normally how the image will look like depends on the physical device and
              it's support over an specific surface, so it is a combination of those
              2 things (physical device, surface)
            - We need to query the device and surface for the following things:
                - available formats
                - available presentation modes
                - extent
        - acquire the image from the presentation engine
        - wait for the image to be available thorugh some synchronization mechanism
        - write to the image and do the pipeline thing
        - release the image either by:
            - giving it back to the presentation engine
            - releasing it manually

Surface Queries
    There are 3 things we can query to check the available configurations between our
    physical device and surface:
        - capabilities
        - format support
        - presentation mode support

